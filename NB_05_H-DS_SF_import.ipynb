{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snowflake Data Import  \n",
    "Import data from GCS to a previously created BigQuery dataset.  \n",
    "\n",
    "This Notebook assumes that you've already generated data at one or more scale factors and uploaded them to the project Google Cloud Storage bucket listed in `config.gcs_data_bucket`  \n",
    "\n",
    "Three values are required to initiate an upload to BigQuery:  \n",
    "1. `test` - the test name, either `h` or `ds`\n",
    "2. `scale` - the scale factor in GB, usually this will be `1, 100, 1000, 10000`  \n",
    "3. `name` - name of this instance of the `test` and `scale` combination, i.e. `time-partitioned`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sf_tpc, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"ds\"\n",
    "scale = 1\n",
    "cid = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sf = sf_tpc.SFTPC(test=test,\n",
    "                  scale=scale,\n",
    "                  cid=cid,\n",
    "                  warehouse=\"TEST9000_XSMALL\",\n",
    "                  desc=\"data_upload\",\n",
    "                  verbose=True,\n",
    "                  verbose_query=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using database: ds_1GB_02XX\n"
     ]
    }
   ],
   "source": [
    "print('Using database:', sf.database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snowflake configuration\n",
      "=======================\n",
      "Username: colin\n",
      "Account:  wja13212\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sf._connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNOWFLAKE QUERY TEXT\n",
      "====================\n",
      "USE ROLE SYSADMIN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sf.role(\"SYSADMIN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNOWFLAKE QUERY TEXT\n",
      "====================\n",
      "CREATE DATABASE IF NOT EXISTS ds_1GB_02XX\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sf.database_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNOWFLAKE QUERY TEXT\n",
      "====================\n",
      "USE DATABASE ds_1GB_02XX\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sf.database_use()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.create_schema({\"ds\": config.fp_sf_ds_schema,\n",
    "                  \"h\": config.fp_sf_h_schema}[test],\n",
    "                 verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNOWFLAKE QUERY TEXT\n",
      "====================\n",
      "DROP INTEGRATION IF EXISTS ds_1GB_02XX_gcs_integration\n",
      "\n",
      "SNOWFLAKE QUERY TEXT\n",
      "====================\n",
      "CREATE STORAGE INTEGRATION ds_1GB_02XX_gcs_integration TYPE=EXTERNAL_STAGE STORAGE_PROVIDER=GCS ENABLED=TRUE STORAGE_ALLOWED_LOCATIONS=('gcs://tpc-benchmark-5947/');\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sf.gcs_integration_drop()\n",
    "sf.gcs_integration_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNOWFLAKE QUERY TEXT\n",
      "====================\n",
      "GRANT CREATE STAGE on schema public to ROLE SYSADMIN;\n",
      "\n",
      "SNOWFLAKE QUERY TEXT\n",
      "====================\n",
      "GRANT USAGE on INTEGRATION ds_1GB_02XX_gcs_integration to ROLE SYSADMIN;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sf.grant_storage_integration_access()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.gcs_inventory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNOWFLAKE QUERY TEXT\n",
      "====================\n",
      "create or replace file format csv_file_format\n",
      "                     type = csv\n",
      "                     field_delimiter = '|'\n",
      "                     skip_header = 0\n",
      "                     null_if = ('NULL', 'null')\n",
      "                     empty_field_as_null = true\n",
      "                     encoding = 'iso-8859-1' \n",
      "                     compression = none;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sf.create_named_file_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNOWFLAKE QUERY TEXT\n",
      "====================\n",
      "USE WAREHOUSE TEST9000_XSMALL\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sf.warehouse_use()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading table: call_center\n",
      "Load SQL:\n",
      "copy into call_center from 'gcs://tpc-benchmark-5947/ds_1GB_call_center_1_4.dat' storage_integration=ds_1GB_02XX_gcs_integration file_format=(format_name=csv_file_format);\n",
      "SNOWFLAKE QUERY TEXT\n",
      "====================\n",
      "copy into call_center from 'gcs://tpc-benchmark-5947/ds_1GB_call_center_1_4.dat' storage_integration=ds_1GB_02XX_gcs_integration file_format=(format_name=csv_file_format);\n",
      "\n",
      "Time Elapsed: 0 days 00:00:06.653380\n",
      "Done!\n",
      "\n",
      "CPU times: user 12 ms, sys: 0 ns, total: 12 ms\n",
      "Wall time: 6.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sf.import_table('call_center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Timestamp('2020-08-08 02:21:22.297043'),\n",
       "  Timestamp('2020-08-08 02:21:28.950423'),\n",
       "  'call_center',\n",
       "  'gcs://tpc-benchmark-5947/ds_1GB_call_center_1_4.dat']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sf.upload_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (<ipython-input-16-6aaf1f276005>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-6aaf1f276005>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "sf.import_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix 1: DS 10000 Scale Factor  \n",
    "This dataset is so big, that the upload can stall. Breaking the upload steps out to restart tables as needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = sf.df_gcs.table.unique()\n",
    "for table in tables: print(table, \"  \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload Log\n",
    "\n",
    "call_center -    \n",
    "catalog_page -   \n",
    "catalog_returns   \n",
    "catalog_sales   \n",
    "customer   \n",
    "customer_address   \n",
    "customer_demographics   \n",
    "date_dim   \n",
    "dbgen_version   \n",
    "household_demographics   \n",
    "income_band   \n",
    "inventory   \n",
    "item   \n",
    "promotion   \n",
    "reason   \n",
    "ship_mode   \n",
    "store   \n",
    "store_returns   \n",
    "store_sales   \n",
    "time_dim   \n",
    "warehouse   \n",
    "web_page   \n",
    "web_returns   \n",
    "web_sales   \n",
    "web_site   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_tables = ['store_sales', 'catalog_sales', 'web_sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# all smaller tables\n",
    "for tb in sf.df_gcs.table.unique():\n",
    "    if tb not in big_tables:\n",
    "        sf.import_table(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# third largest = 'websales'\n",
    "sf.import_table('web_sales')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "# second largest = 'catalog_sales'\n",
    "sf.import_table('catalog_sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional Single Table Upload"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
<<<<<<< Updated upstream
    "sf.import_table(\"reason\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sf.fp_log"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = sf_tpc.parse_log(sf.fp_log)\n",
    "df"
=======
    "# largest = 'websales'\n",
    "sf.import_table('store_sales')"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Table Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sf.import_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sf_tpc.parse_log(sf.fp_log)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
